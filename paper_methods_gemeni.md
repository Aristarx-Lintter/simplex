# Paper Methods: Generating `run_predictions_RCOND_FINAL`

This document outlines the methodology and the code used to generate the analysis results stored in the `run_predictions_RCOND_FINAL/` directory. These results are central to the paper's findings and are generated by a rigorous pipeline involving data preparation, activation extraction, and a cross-validated regression analysis.

## 1. Overview

The primary goal of this analysis is to determine if a linear map exists between a trained neural network's internal activations and the true belief states of the stochastic process it was trained on. The `run_predictions_RCOND_FINAL` directory contains the results of this probing analysis for various models and checkpoints.

The main script orchestrating this process is **`scripts/activation_analysis/do_regression_analysis_FINAL.ipynb`**.

## 2. Methodology

The core of the methodology is a weighted least-squares regression that seeks to find an affine transformation $(\mathbf{L}, \mathbf{b})$ mapping activation vectors $\mathbf{a}$ to ground-truth belief states $\mathbf{\gamma}$:

$$ \mathbf{\gamma} \approx \mathbf{L} \mathbf{a} + \mathbf{b} $$

A crucial aspect of this is the regularization of the linear map, controlled by the `rcond` hyperparameter. This parameter is tuned using K-fold cross-validation to prevent overfitting and ensure the discovered linear map is robust.

### Key Steps:

1.  **Data Loading & Preparation**:
    *   Models, configurations, and training data are loaded from S3 using `epsilon_transformers.analysis.load_data.S3ModelLoader`.
    *   Ground-truth belief states (for both the quantum processes and classical Markov controls) and their corresponding probabilities are generated. This is handled by `epsilon_transformers.analysis.activation_analysis.prepare_msp_data` and `scripts.activation_analysis.belief_states.BeliefStateGenerator`.
    *   **Deduplication**: A critical step is the deduplication of data points. Many input sequences can lead to the same belief state. The code identifies these duplicate prefixes and aggregates their probabilities. This is performed by the `deduplicate_data` function in the notebook.

2.  **Activation Extraction**:
    *   For each model checkpoint, internal activations are extracted. The `scripts.activation_analysis.data_loading.ActivationExtractor` class is used to hook into the model and capture these activations.
    *   Activations are extracted from multiple layers, as specified in `scripts.activation_analysis.config.TRANSFORMER_ACTIVATION_KEYS`. A special `'combined'` activation is also created by concatenating all layer activations.

3.  **Cross-Validated Regression**:
    *   The central analysis is performed by the **`run_activation_to_beliefs_regression_kf`** function from `scripts.activation_analysis.regression.py`.
    *   This function implements a 10-fold cross-validation to find the optimal `rcond` value. `rcond` is a regularization parameter for the pseudoinverse calculation (`torch.pinverse`) that prevents overfitting.
    *   The range of `rcond` values to test is defined by `RCOND_SWEEP_LIST` in `scripts.activation_analysis.config.py`.
    *   For each fold, the regression model is trained on the training set and evaluated on the test set for every `rcond` value.
    *   The `rcond` that yields the lowest average error across all folds is selected as the optimal hyperparameter.

4.  **Final Model Training & Prediction**:
    *   After identifying the best `rcond` via cross-validation, a final regression model is trained on the *entire* dataset using this optimal `rcond`.
    *   This final model is then used to generate the predicted belief states from the activations.

5.  **Output Generation**:
    *   The results are saved into the `run_predictions_RCOND_FINAL/` directory.
    *   The directory is structured as: `run_predictions_RCOND_FINAL/{sweep_id}_{run_id_int}/`.
    *   Inside each run's directory, the following key files are generated:
        *   `ground_truth_data.joblib`: Contains the deduplicated ground-truth beliefs and their probabilities.
        *   `markov3_ground_truth_data.joblib`: Same as above, but for the classical Markov-3 process.
        *   `checkpoint_{ckpt_ind}.joblib`: Contains the results for a specific model checkpoint. This file holds the predicted beliefs, error metrics (RMSE, RÂ², etc.), and other analysis artifacts for each layer.
        *   `markov3_checkpoint_{ckpt_ind}.joblib`: Same as above, but for the classical Markov-3 process.

## 3. Relevant Files

*   **Orchestration Notebook**: `scripts/activation_analysis/do_regression_analysis_FINAL.ipynb`
*   **Core Regression Logic**: `scripts.activation_analysis/regression.py` (especially `run_activation_to_beliefs_regression_kf` and `_train_final_model`)
*   **Configuration**: `scripts.activation_analysis/config.py` (defines `RCOND_SWEEP_LIST`, model layers, etc.)
*   **Data Loading**: `epsilon_transformers.analysis.load_data.S3ModelLoader`, `scripts.activation_analysis.data_loading.ActivationExtractor`
*   **Methodology Description**: `probing_methodology_neurips.md` provides a high-level theoretical description of the probing method.
