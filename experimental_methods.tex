\section{Experimental Methods}

\subsection{Experimental Design}

We conduct a comprehensive evaluation of four neural network architectures on four distinct stochastic processes, resulting in 16 experimental configurations. The architectures include transformers, LSTMs, GRUs, and vanilla RNNs, while the processes consist of Mess3 (classical), FRDN (quantum), Bloch Walk (quantum), and the Moon Process (post-quantum)---each representing different computational complexity classes as described in Section~\ref{sec:processes}.

\subsection{Model Architectures}

For the Transformer architecture, we employ a 4-layer model implemented using the TransformerLens framework~\citep{nanda2022transformerlens}. The model uses multi-head attention with 4 heads (dimension 16 per head), 64-dimensional embeddings, and a 256-dimensional feed-forward network with ReLU activation. Layer normalization is applied before each sub-layer, and the model processes fixed sequences of 8 tokens with learned positional embeddings.

We compare three RNN variants, each configured with identical hyperparameters to ensure fair comparison: 4 recurrent layers with 64 hidden units per layer, unidirectional processing, one-hot input encoding, and a linear output projection. The variants differ only in their gating mechanisms: LSTM uses forget, input, and output gates; GRU employs reset and update gates; and the vanilla RNN uses simple tanh activation without gating.

\subsection{Training Methodology}

Training data is generated from each stochastic process with the following parameters:
\begin{itemize}
    \item \textbf{Mess3}: $a=0.85$, $x=0.05$
    \item \textbf{Fanizza}: $\alpha=2000.0$, $\lambda=0.49$  
    \item \textbf{Tom Quantum}: $\alpha=1.0$, $\beta=7.14$
    \item \textbf{Post Quantum}: $\alpha=2.72$, $\beta=0.5$
\end{itemize}

Each training example consists of an 8-token input sequence with corresponding next-token targets for each position. All experiments use consistent random seeding (seed=42) for reproducibility.

\subsubsection{Training Hyperparameters}

Table~\ref{tab:hyperparams} shows the core hyperparameters used across all experiments. All models are trained for 20,000 epochs using the Adam optimizer with learning rate $1 \times 10^{-4}$.

\begin{table}[h]
\centering
\caption{Core hyperparameters used across all experiments}
\label{tab:hyperparams}
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
Optimizer & Adam ($\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=10^{-8}$) \\
Learning rate & $1 \times 10^{-4}$ \\
Weight decay & None \\
Gradient clipping & None \\
Epochs & 20,000 \\
Validation frequency & Every epoch \\
Checkpoint frequency & Every 100 epochs \\
Random seed & 42 \\
\bottomrule
\end{tabular}
\end{table}

The majority of experiments use the following configuration:

\begin{table}[h]
\centering
\caption{Standard configuration used by most experiments}
\label{tab:standard_config}
\begin{tabular}{ll}
\toprule
\textbf{Configuration} & \textbf{Standard Setting} \\
\midrule
Batch size & 128 \\
Batches per epoch & 200 \\
LR scheduler & ReduceLROnPlateau$^*$ \\
Total checkpoints & 201 \\
\bottomrule
\end{tabular}
\end{table}

\noindent $^*$ReduceLROnPlateau parameters: factor=0.5, patience=1000, cooldown=200, threshold=$10^{-6}$

\subsubsection{Experiment-Specific Variations}

While all RNN variants (LSTM, GRU, RNN) use the standard configuration above for all processes, we found that certain transformer experiments trained better with modified settings:

\begin{table}[h]
\centering
\caption{Experiment-specific variations from standard configuration}
\label{tab:variations}
\begin{tabular}{lccc}
\toprule
\textbf{Experiment} & \textbf{Batch Size} & \textbf{Batches/Epoch} & \textbf{LR Scheduler} \\
\midrule
Transformer-Fanizza & 16 & 20 & None \\
Transformer-Post Quantum & 16 & 20 & ReduceLROnPlateau \\
\bottomrule
\end{tabular}
\end{table}

We train using standard cross-entropy loss. Validation is performed every epoch on the full dataset, with model checkpoints saved every 100 epochs (201 total) and comprehensive metric logging via Weights \& Biases.

\subsection{Implementation Details}

All experiments are implemented in PyTorch 2.0 with CUDA acceleration, using FP32 precision throughout. Training is distributed across multiple GPUs, with specific GPU assignments managed through a parallel execution framework. To ensure reproducibility, we use fixed random seeds, deterministic data generation, consistent initialization schemes, and version-controlled configuration files.

Several key methodological decisions guide our experimental design. We maintain 4 layers across all architectures to ensure fair comparison of inductive biases rather than capacity differences. The high checkpoint frequency (every 100 epochs) enables detailed analysis of learning dynamics and convergence behavior. Finally, we deliberately avoid dropout, weight decay, or other regularization techniques to study the pure learning dynamics of each architecture on these processes.