
\section{Experimental Setup}

\subsection{Models}
We use two main types of models in our experiments: Transformers and Recurrent Neural Networks (RNNs).

\subsubsection{Transformer}
The transformer models are based on the \texttt{HookedTransformer} architecture from the \texttt{transformer_lens} library. The key hyperparameters for the transformer models are:
\begin{itemize}
    \item Number of layers (\texttt{n_layers})
    \item Model dimension (\texttt{d_model})
    \item Head dimension (\texttt{d_head})
    \item Number of heads (\texttt{n_heads})
    \item MLP dimension (\texttt{d_mlp})
    \item Vocabulary size (\texttt{d_vocab})
    \item Context length (\texttt{n_ctx})
    \item Activation function (\texttt{act_fn})
    \item Normalization type (\texttt{normalization_type})
\end{itemize}

\subsubsection{RNN}
The RNN models are implemented as a wrapper around PyTorch's \texttt{nn.RNN}, \texttt{nn.LSTM}, or \texttt{nn.GRU} modules. The hyperparameters for the RNN models are:
\begin{itemize}
    \item RNN type (\texttt{rnn_type}: RNN, LSTM, or GRU)
    \item Hidden size (\texttt{hidden_size})
    \item Number of layers (\texttt{num_layers})
    \item Dropout (\texttt{dropout})
    \item Bidirectional (\texttt{bidirectional})
\end{itemize}

\subsection{Training}
The models are trained using the Adam optimizer with a configurable learning rate. A \texttt{ReduceLROnPlateau} learning rate scheduler is used to adjust the learning rate based on the validation loss. The loss function is cross-entropy.

The training is performed for a fixed number of epochs. The training data is either sampled in batches or the entire dataset is used in each epoch. Validation is performed periodically to monitor the model's performance.

\subsection{Datasets}
The training data is generated from a variety of stochastic processes. Each process is defined by a set of transition matrices. The following processes are used in our experiments:
\begin{itemize}
    \item \textbf{Post Quantum}: A process with parameters $\alpha$ and $\beta$.
    \item \textbf{Tom Quantum}: A process with parameters $\alpha$ and $\beta$.
    \item \textbf{Fanizza}: A process with parameters $\alpha$ and $\lambda$.
    \item \textbf{RRXOR}: A process with parameters $p_{R1}$ and $p_{R2}$.
    \item \textbf{MESS3}: A process with parameters $x$ and $a$.
    \item \textbf{Days of the Week}: A process with no parameters.
    \item \textbf{Zero-One Random}: A process with parameter $p$.
\end{itemize}

\subsection{Hyperparameters}
The following table summarizes the hyperparameters used for the different experiments. All RNN-based models were trained with a batch size of 128 for 20,000 epochs, using the Adam optimizer with a learning rate of 0.0001. The context length was set to 8.

\begin{table}[H]
\centering
\caption{Hyperparameters for the different experiments.}
\label{tab:hyperparams}
\begin{tabular}{l|l|l|c|c}
\toprule
\textbf{Process} & \textbf{Model Type} & \textbf{Process Parameters} & \textbf{Layers} & \textbf{Hidden Size} \\
\midrule
\multirow{3}{*}{Mess3} & LSTM & a=0.85, x=0.05 & 4 & 64 \\
& GRU & a=0.85, x=0.05 & 4 & 64 \\
& RNN & a=0.85, x=0.05 & 4 & 64 \\
\hline
\multirow{3}{*}{Fanizza} & LSTM & $\alpha$=2000.0, $\lambda$=0.49 & 4 & 64 \\
& GRU & $\alpha$=2000.0, $\lambda$=0.49 & 4 & 64 \\
& RNN & $\alpha$=2000.0, $\lambda$=0.49 & 4 & 64 \\
\hline
\multirow{3}{*}{Tom Quantum} & LSTM & $\alpha$=1.0, $\beta$=7.14 & 4 & 64 \\
& GRU & $\alpha$=1.0, $\beta$=7.14 & 4 & 64 \\
& RNN & $\alpha$=1.0, $\beta$=7.14 & 4 & 64 \\
\hline
\multirow{3}{*}{Post Quantum} & LSTM & $\alpha$=2.72, $\beta$=0.5 & 4 & 64 \\
& GRU & $\alpha$=2.72, $\beta$=0.5 & 4 & 64 \\
& RNN & $\alpha$=2.72, \beta=0.5 & 4 & 64 \\
\bottomrule
\end{tabular}
\end{table}

The Transformer models failed to load due to a configuration error and are not included in this table.