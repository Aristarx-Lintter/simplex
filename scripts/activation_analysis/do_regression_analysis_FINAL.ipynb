{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 13:16:22,025 - root - INFO - Logging session started. Log file: logs/analysis_20250422_131622.log\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from epsilon_transformers.analysis.load_data import S3ModelLoader\n",
    "from epsilon_transformers.analysis.activation_analysis import prepare_msp_data\n",
    "from scripts.activation_analysis.data_loading import ActivationExtractor\n",
    "from scripts.activation_analysis.config import TRANSFORMER_ACTIVATION_KEYS\n",
    "from scripts.activation_analysis.config import RCOND_SWEEP_LIST\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import numpy as np # Make sure numpy is imported\n",
    "import os # Make sure os is imported\n",
    "\n",
    "from scripts.activation_analysis.regression import (\n",
    "    RegressionAnalyzer,\n",
    "    run_single_rcond_sweep_with_predictions,\n",
    "    run_single_rcond_sweep_with_predictions_flat,\n",
    "    run_paul_rcond_sweep_with_sklearn_predictions_flat,\n",
    "    run_activation_to_beliefs_regression_kf,\n",
    "    run_activation_to_beliefs_regression_ridgecv,\n",
    "    run_activation_to_beliefs_regression_pca\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _combine_layer_activations(nn_acts):\n",
    "    \"\"\"Combine activations from all layers into a single tensor, by concatenating them\n",
    "    make sure to reshape back to the original shape\n",
    "    a single act is of shape (n_samples, n_ctx, d_model)\n",
    "    so in the end the shape should be (n_samples, n_ctx, d_model*n_layers)\"\"\"\n",
    "    flattened_acts = []\n",
    "    first_layer_key = list(nn_acts.keys())[0]\n",
    "    n_samples = nn_acts[first_layer_key].shape[0]\n",
    "    n_ctx = nn_acts[first_layer_key].shape[1]\n",
    "    \n",
    "    all_acts = []\n",
    "    for layer_act in nn_acts.values():\n",
    "        # Reshape to (n_samples, -1) to flatten any extra dimensions\n",
    "        all_acts.append(layer_act)\n",
    "\n",
    "    cat = torch.cat(all_acts, dim=2) \n",
    "    #print(cat.shape)\n",
    "    return cat\n",
    "\n",
    "\n",
    "\n",
    "def find_duplicate_prefixes(nn_inputs):\n",
    "    \"\"\"\n",
    "    Find duplicate prefixes in the input sequences and return their indices.\n",
    "    \n",
    "    Args:\n",
    "        nn_inputs: Tensor of shape (batch_size, seq_len) containing token sequences\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping each unique prefix tuple to a list of (seq_idx, pos) tuples\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = nn_inputs.shape\n",
    "    prefix_to_indices = {}  # (prefix tuple) -> list of (seq_idx, pos) tuples\n",
    "    \n",
    "    # Process each sequence and position\n",
    "    for seq_idx in range(batch_size):\n",
    "        seq = nn_inputs[seq_idx]\n",
    "        \n",
    "        for pos in range(seq_len):\n",
    "            # Get the prefix up to this position\n",
    "            prefix = tuple(seq[:pos+1].cpu().numpy().tolist())\n",
    "            \n",
    "            # Add this occurrence to our mapping\n",
    "            if prefix not in prefix_to_indices:\n",
    "                prefix_to_indices[prefix] = []\n",
    "            prefix_to_indices[prefix].append((seq_idx, pos))\n",
    "    \n",
    "    return prefix_to_indices\n",
    "\n",
    "def combine_duplicate_data(prefix_to_indices, activations, nn_probs, belief_states, debug=False, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Combine data for duplicate prefixes by summing probabilities.\n",
    "    \n",
    "    Args:\n",
    "        prefix_to_indices: Dictionary mapping prefixes to lists of (seq_idx, pos) tuples\n",
    "        activations: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        nn_probs: Tensor of shape (batch_size, seq_len)\n",
    "        belief_states: Tensor of shape (batch_size, seq_len, belief_dim)\n",
    "        debug: Whether to print debug information\n",
    "        tolerance: Tolerance for activation differences\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (unique_activations, summed_probs, unique_beliefs, unique_prefixes)\n",
    "    \"\"\"\n",
    "    # Dictionary to store unique activations and summed probabilities\n",
    "    unique_data = {}  # (prefix tuple) -> (activation, summed_prob, belief_state, count)\n",
    "    \n",
    "    # Debug information\n",
    "    if debug:\n",
    "        activation_diffs = {}  # (prefix tuple) -> max difference observed\n",
    "        inconsistent_prefixes = []\n",
    "    \n",
    "    # Process each unique prefix\n",
    "    for prefix, indices_list in prefix_to_indices.items():\n",
    "        first_seq_idx, first_pos = indices_list[0]\n",
    "        act = activations[first_seq_idx, first_pos]\n",
    "        prob = nn_probs[first_seq_idx, first_pos]\n",
    "        belief = belief_states[first_seq_idx, first_pos]\n",
    "        count = 1\n",
    "        \n",
    "        # Process additional occurrences of this prefix\n",
    "        for seq_idx, pos in indices_list[1:]:\n",
    "            current_act = activations[seq_idx, pos]\n",
    "            current_prob = nn_probs[seq_idx, pos]\n",
    "            \n",
    "            if debug:\n",
    "                # Check if activations are the same\n",
    "                diff = torch.max(torch.abs(act - current_act)).item()\n",
    "                \n",
    "                if diff > tolerance:\n",
    "                    if prefix not in inconsistent_prefixes:\n",
    "                        inconsistent_prefixes.append(prefix)\n",
    "                    \n",
    "                    current_max_diff = activation_diffs.get(prefix, 0)\n",
    "                    activation_diffs[prefix] = max(current_max_diff, diff)\n",
    "            \n",
    "            # Sum the probabilities\n",
    "            prob += current_prob\n",
    "            count += 1\n",
    "        \n",
    "        # Store the combined data\n",
    "        unique_data[prefix] = (act, prob, belief, count)\n",
    "    \n",
    "    # Print debug info if requested\n",
    "    if debug:\n",
    "        if inconsistent_prefixes:\n",
    "            print(f\"WARNING: Found {len(inconsistent_prefixes)} prefixes with inconsistent activations!\")\n",
    "            print(f\"Max differences observed:\")\n",
    "            for prefix in sorted(inconsistent_prefixes, key=lambda p: activation_diffs[p], reverse=True)[:10]:\n",
    "                print(f\"  Prefix {prefix}: max diff = {activation_diffs[prefix]}, seen {unique_data[prefix][3]} times\")\n",
    "        else:\n",
    "            print(\"All prefixes have consistent activations (within tolerance).\")\n",
    "            \n",
    "        total_saved = sum(unique_data[p][3] - 1 for p in unique_data)\n",
    "        total_items = sum(len(indices) for indices in prefix_to_indices.values())\n",
    "        print(f\"Deduplication saved {total_saved} activations out of {total_items}\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    unique_prefixes = list(unique_data.keys())\n",
    "    unique_activations = torch.stack([unique_data[p][0] for p in unique_prefixes])\n",
    "    summed_probs = torch.tensor([unique_data[p][1] for p in unique_prefixes])\n",
    "    unique_beliefs = torch.stack([unique_data[p][2] for p in unique_prefixes])\n",
    "    \n",
    "    return unique_activations, summed_probs, unique_beliefs, unique_prefixes\n",
    "\n",
    "def deduplicate_tensor(prefix_to_indices, tensor, aggregation_fn=None, debug=False, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Deduplicate a single tensor based on prefix indices.\n",
    "    \n",
    "    Args:\n",
    "        prefix_to_indices: Dictionary mapping prefixes to lists of (seq_idx, pos) tuples\n",
    "        tensor: Tensor of shape (batch_size, seq_len, ...) to deduplicate\n",
    "        aggregation_fn: Function to aggregate duplicate values (default: take first occurrence)\n",
    "                        Should accept a list of tensor values and return a single value\n",
    "        debug: Whether to print debug information\n",
    "        tolerance: Tolerance for tensor value differences\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (unique_tensor_values, unique_prefixes)\n",
    "    \"\"\"\n",
    "    # Dictionary to store unique tensor values\n",
    "    unique_data = {}  # (prefix tuple) -> (tensor_value, count)\n",
    "    \n",
    "    # Debug information\n",
    "    if debug:\n",
    "        value_diffs = {}  # (prefix tuple) -> max difference observed\n",
    "        inconsistent_prefixes = []\n",
    "    \n",
    "    # Process each unique prefix\n",
    "    for prefix, indices_list in prefix_to_indices.items():\n",
    "        first_seq_idx, first_pos = indices_list[0]\n",
    "        value = tensor[first_seq_idx, first_pos]\n",
    "        count = 1\n",
    "        \n",
    "        # If we have an aggregation function and multiple occurrences, prepare to aggregate\n",
    "        if aggregation_fn is not None and len(indices_list) > 1:\n",
    "            values = [value]\n",
    "            \n",
    "            # Collect all values for this prefix\n",
    "            for seq_idx, pos in indices_list[1:]:\n",
    "                current_value = tensor[seq_idx, pos]\n",
    "                values.append(current_value)\n",
    "                \n",
    "                if debug:\n",
    "                    # Check if values are the same\n",
    "                    diff = torch.max(torch.abs(value - current_value)).item()\n",
    "                    \n",
    "                    if diff > tolerance:\n",
    "                        if prefix not in inconsistent_prefixes:\n",
    "                            inconsistent_prefixes.append(prefix)\n",
    "                        \n",
    "                        current_max_diff = value_diffs.get(prefix, 0)\n",
    "                        value_diffs[prefix] = max(current_max_diff, diff)\n",
    "                \n",
    "                count += 1\n",
    "            \n",
    "            # Aggregate the values\n",
    "            value = aggregation_fn(values)\n",
    "        \n",
    "        # Store the unique value\n",
    "        unique_data[prefix] = (value, count)\n",
    "    \n",
    "    # Print debug info if requested\n",
    "    if debug:\n",
    "        if inconsistent_prefixes:\n",
    "            print(f\"WARNING: Found {len(inconsistent_prefixes)} prefixes with inconsistent values!\")\n",
    "            print(f\"Max differences observed:\")\n",
    "            for prefix in sorted(inconsistent_prefixes, key=lambda p: value_diffs[p], reverse=True)[:10]:\n",
    "                print(f\"  Prefix {prefix}: max diff = {value_diffs[prefix]}, seen {unique_data[prefix][1]} times\")\n",
    "        else:\n",
    "            print(\"All prefixes have consistent values (within tolerance).\")\n",
    "            \n",
    "        total_saved = sum(unique_data[p][1] - 1 for p in unique_data)\n",
    "        total_items = sum(len(indices) for indices in prefix_to_indices.values())\n",
    "        print(f\"Deduplication saved {total_saved} items out of {total_items}\")\n",
    "    \n",
    "    # Convert to tensor\n",
    "    unique_prefixes = list(unique_data.keys())\n",
    "    unique_values = torch.stack([unique_data[p][0] for p in unique_prefixes])\n",
    "    \n",
    "    return unique_values, unique_prefixes\n",
    "\n",
    "def get_nn_type(run_id):\n",
    "    if 'GRU' in run_id or 'LSTM' in run_id or 'RNN' in run_id or 're' in run_id:\n",
    "        return \"RNN\"\n",
    "    else:\n",
    "        return \"transformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epsilon_transformers.analysis.load_data import S3ModelLoader\n",
    "s3_loader = S3ModelLoader(use_company_credentials=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20241121152808', '20241205175736', '20250304052839', '20250304060315', '20250304063704', '20250304070751', '20250304073843', '20250304075926', '20250327113715', '20250327115247', '20250327131902', '20250421221507', '20250422023003']\n",
      "['run_0_L1_H4_DH16_DM64_post_quantum', 'run_10_L2_H4_DH16_DM64_tom_quantum', 'run_11_L2_H4_DH16_DM64_tom_quantum', 'run_12_L2_H4_DH16_DM64_tom_quantum', 'run_13_L2_H4_DH16_DM64_fanizza', 'run_14_L2_H4_DH16_DM64_rrxor', 'run_15_L2_H4_DH16_DM64_mess3', 'run_16_L4_H4_DH16_DM64_post_quantum', 'run_17_L4_H4_DH16_DM64_tom_quantum', 'run_18_L4_H4_DH16_DM64_tom_quantum', 'run_19_L4_H4_DH16_DM64_tom_quantum', 'run_1_L1_H4_DH16_DM64_tom_quantum', 'run_20_L4_H4_DH16_DM64_tom_quantum', 'run_21_L4_H4_DH16_DM64_fanizza', 'run_22_L4_H4_DH16_DM64_rrxor', 'run_23_L4_H4_DH16_DM64_mess3', 'run_2_L1_H4_DH16_DM64_tom_quantum', 'run_3_L1_H4_DH16_DM64_tom_quantum', 'run_4_L1_H4_DH16_DM64_tom_quantum', 'run_5_L1_H4_DH16_DM64_fanizza', 'run_6_L1_H4_DH16_DM64_rrxor', 'run_7_L1_H4_DH16_DM64_mess3', 'run_8_L2_H4_DH16_DM64_post_quantum', 'run_9_L2_H4_DH16_DM64_tom_quantum']\n"
     ]
    }
   ],
   "source": [
    "sweeps = s3_loader.list_sweeps()\n",
    "print(sweeps)\n",
    "SWEEP = sweeps[1]\n",
    "runs = s3_loader.list_runs_in_sweep(SWEEP)\n",
    "print(runs)\n",
    "RUN = 'run_21_L4_H4_DH16_DM64_fanizza'\n",
    "RUN = 'run_16_L4_H4_DH16_DM64_post_quantum'\n",
    "RUN = 'run_17_L4_H4_DH16_DM64_tom_quantum'\n",
    "#RUN = 'run_22_L4_H4_DH16_DM64_rrxor'\n",
    "#RUN = 'run_23_L4_H4_DH16_DM64_mess3'\n",
    "\n",
    "CHECKPOINT_NUM = -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings # To handle potential warnings during PCA\n",
    "\n",
    "def deduplicate_data(inputs, probs, beliefs):\n",
    "    \"\"\"\n",
    "    Deduplicate inputs, probs, and beliefs based on duplicate prefixes.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input tensor to find duplicate prefixes\n",
    "        probs: Probability tensor to deduplicate\n",
    "        beliefs: Belief tensor to deduplicate\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (deduplicated probs, deduplicated beliefs, deduplicated indices)\n",
    "    \"\"\"\n",
    "    prefix_to_indices = find_duplicate_prefixes(inputs)\n",
    "    dedup_probs, dedup_indices = deduplicate_tensor(prefix_to_indices, probs, aggregation_fn=sum)\n",
    "    # normalize the probs to sum to 1\n",
    "    dedup_probs = dedup_probs / dedup_probs.sum()\n",
    "    dedup_beliefs, _ = deduplicate_tensor(prefix_to_indices, beliefs, aggregation_fn=None)\n",
    "    return dedup_probs, dedup_beliefs, dedup_indices, prefix_to_indices\n",
    "\n",
    "def calculate_weighted_pca_variance(activations: np.ndarray, weights: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs weighted PCA and calculates explained variance ratios.\n",
    "\n",
    "    Args:\n",
    "        activations (np.ndarray): The data array (e.g., deduplicated activations) \n",
    "                                  of shape (N, D), where N is number of samples \n",
    "                                  and D is number of features.\n",
    "        weights (np.ndarray): The weights array (e.g., deduplicated probabilities) \n",
    "                              of shape (N,). Weights should be non-negative.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - cumulative_explained_variance (np.ndarray | None): \n",
    "                Array of cumulative explained variance (shape D,). Returns None if PCA fails.\n",
    "            - explained_variance_ratio (np.ndarray | None): \n",
    "                Array of explained variance ratio per component (shape D,). Returns None if PCA fails.\n",
    "            - sorted_eigenvalues (np.ndarray | None): \n",
    "                Array of sorted eigenvalues (shape D,). Returns None if PCA fails.\n",
    "    \"\"\"\n",
    "    if activations.shape[0] != weights.shape[0]:\n",
    "        raise ValueError(f\"Number of samples mismatch: activations ({activations.shape[0]}) vs weights ({weights.shape[0]})\")\n",
    "    if np.any(weights < 0):\n",
    "        warnings.warn(\"Input weights contain negative values.\", RuntimeWarning)\n",
    "    if not np.isclose(np.sum(weights), 1.0):\n",
    "        warnings.warn(\"Input weights do not sum close to 1. Normalizing weights for PCA.\", RuntimeWarning)\n",
    "        weights = weights / np.sum(weights)\n",
    "        # Ensure no division by zero if sum was zero\n",
    "        if np.isnan(weights).any():\n",
    "             weights = np.ones_like(weights) / weights.shape[0]\n",
    "\n",
    "\n",
    "    try:\n",
    "        # 1. Calculate weighted mean\n",
    "        weighted_mean_acts = np.average(activations, axis=0, weights=weights)\n",
    "        \n",
    "        # 2. Center the activation data\n",
    "        centered_acts = activations - weighted_mean_acts\n",
    "        \n",
    "        # 3. Compute the weighted covariance matrix\n",
    "        # Use bias=True for population estimate, typical for PCA\n",
    "        # Use ddof=0 explicitly which is equivalent to bias=True for np.cov\n",
    "        cov_matrix = np.cov(centered_acts, rowvar=False, aweights=weights, ddof=0) \n",
    "        \n",
    "        # 4. Eigen decomposition (use eigh for symmetric matrix)\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "        \n",
    "        # Check for complex eigenvalues (shouldn't happen for covariance, but safety check)\n",
    "        if np.iscomplexobj(eigenvalues):\n",
    "            warnings.warn(\"Complex eigenvalues encountered. Taking real part.\", RuntimeWarning)\n",
    "            eigenvalues = eigenvalues.real\n",
    "        \n",
    "        # Check for negative eigenvalues (can happen due to numerical instability)\n",
    "        if np.any(eigenvalues < -1e-10): # Allow for small negative noise\n",
    "            negative_count = np.sum(eigenvalues < -1e-10)\n",
    "            warnings.warn(f\"{negative_count} negative eigenvalues encountered. Setting them to zero.\", RuntimeWarning)\n",
    "            eigenvalues[eigenvalues < 0] = 0\n",
    "\n",
    "        # 5. Sort eigenvalues and eigenvectors in descending order\n",
    "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "        sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "        # sorted_eigenvectors = eigenvectors[:, sorted_indices] # Eigenvectors not always needed\n",
    "        \n",
    "        # 6. Calculate explained variance ratio\n",
    "        total_variance = np.sum(sorted_eigenvalues)\n",
    "        \n",
    "        if total_variance <= 1e-10: # Handle case of zero variance\n",
    "             warnings.warn(\"Total variance is close to zero. Explained variance cannot be computed.\", RuntimeWarning)\n",
    "             explained_variance_ratio = np.zeros_like(sorted_eigenvalues)\n",
    "             cumulative_explained_variance = np.zeros_like(sorted_eigenvalues)\n",
    "        else:\n",
    "            explained_variance_ratio = sorted_eigenvalues / total_variance\n",
    "            # 7. Calculate cumulative explained variance\n",
    "            cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "        return cumulative_explained_variance, explained_variance_ratio, sorted_eigenvalues\n",
    "\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        warnings.warn(f\"Weighted PCA failed due to LinAlgError: {e}. Returning None.\", RuntimeWarning)\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "         warnings.warn(f\"Unexpected error during weighted PCA: {e}. Returning None.\", RuntimeWarning)\n",
    "         return None, None, None\n",
    "    \n",
    "import collections # Or 'from collections import defaultdict' if not already done\n",
    "\n",
    "def nested_dict_factory():\n",
    "    \"\"\"Returns a defaultdict that defaults to a regular dictionary.\"\"\"\n",
    "    return collections.defaultdict(dict)\n",
    "\n",
    "def compute_kfold_split(flat_probs, n_splits=5, random_state=42):\n",
    "    from sklearn.model_selection import KFold\n",
    "    import numpy as np\n",
    "\n",
    "    # If flat_probs is a PyTorch tensor, convert it to numpy\n",
    "    if isinstance(flat_probs, torch.Tensor):\n",
    "        flat_probs = flat_probs.cpu().detach().numpy()\n",
    "    \n",
    "    # Create position indices\n",
    "    all_positions = np.arange(len(flat_probs))\n",
    "    \n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "\n",
    "    \n",
    "    # Return the KFold object and positions\n",
    "    # kf is a list of tuples, each tuple contains two lists: the indices of the training set and the indices of the test set\n",
    "    return kf, all_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_55_L4_H64_LSTM_uni_mess3\n",
      "Computing MSP data...\n",
      "Computing MSP data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32dd1f7a1b8462bb98483b2cffb6d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best rcond from CV: 4.498432645050343e-06, with avg error: 0.075692\n",
      "Training final model with SVD using rcond=4.498432645050343e-06\n",
      "Best rcond from CV: 4.498432645050343e-06, with avg error: 0.427064\n",
      "Training final model with SVD using rcond=4.498432645050343e-06\n",
      "Best rcond from CV: 1.3894955372961704e-06, with avg error: 0.092881\n",
      "Training final model with SVD using rcond=1.3894955372961704e-06\n",
      "Best rcond from CV: 1.757510631250625e-06, with avg error: 0.502255\n",
      "Training final model with SVD using rcond=1.757510631250625e-06\n",
      "Best rcond from CV: 2.1209508815900335e-07, with avg error: 0.113759\n",
      "Training final model with SVD using rcond=2.1209508815900335e-07\n",
      "Best rcond from CV: 2.682695878775121e-07, with avg error: 0.573182\n",
      "Training final model with SVD using rcond=2.682695878775121e-07\n",
      "Best rcond from CV: 3.393221845726657e-07, with avg error: 0.141859\n",
      "Training final model with SVD using rcond=3.393221845726657e-07\n",
      "Best rcond from CV: 1.3257113096187823e-07, with avg error: 0.656062\n",
      "Training final model with SVD using rcond=1.3257113096187823e-07\n",
      "Best rcond from CV: 1e-05, with avg error: 0.256461\n",
      "Training final model with SVD using rcond=1e-05\n",
      "Best rcond from CV: 8.286427544135222e-08, with avg error: 0.814626\n",
      "Training final model with SVD using rcond=8.286427544135222e-08\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scripts.activation_analysis.data_loading import ModelDataManager\n",
    "from scripts.activation_analysis.belief_states import BeliefStateGenerator\n",
    "model_data_manager = ModelDataManager(device='cpu', use_company_s3=True)\n",
    "belief_generator = BeliefStateGenerator(model_data_manager, device='cpu')\n",
    "\n",
    "reg_analyzer = RegressionAnalyzer(device='cpu', use_efficient_pinv=True)\n",
    "\n",
    "output_dir = \"run_predictions_RCOND_FINAL\"\n",
    "os.makedirs(output_dir, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "sweep_run_pairs = [\n",
    "    # Mess3\n",
    "    (\"20241121152808\", 55),  # LSTM\n",
    "    (\"20241205175736\", 23),  # Transformer\n",
    "    \n",
    "    # Fanizza\n",
    "    (\"20241121152808\", 53),  # LSTM\n",
    "    (\"20250422023003\", 1),  # Transformer\n",
    "    \n",
    "    # Tom Quantum A\n",
    "    (\"20241121152808\", 49),  # LSTM\n",
    "    (\"20241205175736\", 17),  # Transformer\n",
    "    \n",
    "    \n",
    "    # Post Quantum\n",
    "    (\"20241121152808\", 48),  # LSTM\n",
    "    (\"20250421221507\", 0),  # Transformer\n",
    "]\n",
    "\n",
    "\n",
    "for sweep, run_id_int in sweep_run_pairs:\n",
    "    run_dir = f'{output_dir}/{sweep}_{run_id_int}'\n",
    "    os.makedirs(run_dir, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "    runs = s3_loader.list_runs_in_sweep(sweep)\n",
    "    # keep the entry of run that has f'run_{run_id_int}' in it\n",
    "    run_id = [x for x in runs if f'run_{run_id_int}' in x][0]\n",
    "\n",
    "    print(run_id)\n",
    "    ckpts = s3_loader.list_checkpoints(sweep, run_id)\n",
    "    model, run_config = s3_loader.load_checkpoint(sweep, run_id, ckpts[0])\n",
    "\n",
    "\n",
    "    loss_df = s3_loader.load_loss_from_run(sweep, run_id)\n",
    "\n",
    "    n_ctx = run_config[\"model_config\"][\"n_ctx\"]\n",
    "    run_config[\"n_ctx\"] = n_ctx\n",
    "    nn_inputs, nn_beliefs, _, nn_probs, _ = prepare_msp_data(\n",
    "        run_config, run_config[\"process_config\"]\n",
    "    )\n",
    "\n",
    "    classical_beliefs = belief_generator.generate_classical_belief_states(\n",
    "    run_config, max_order=3)\n",
    "\n",
    "    classical_nn_inputs = classical_beliefs['markov_order_3']['inputs']\n",
    "    classical_nn_beliefs = classical_beliefs['markov_order_3']['beliefs']\n",
    "    classical_nn_probs = classical_beliefs['markov_order_3']['probs']\n",
    "    \n",
    "    # Deduplicate neural network data\n",
    "    dedup_probs, dedup_beliefs, dedup_indices, prefix_to_indices = deduplicate_data(\n",
    "        nn_inputs, \n",
    "        nn_probs, \n",
    "        nn_beliefs\n",
    "    )\n",
    "\n",
    "    kf, all_positions = compute_kfold_split(dedup_probs, n_splits=100, random_state=42)\n",
    "    kf_list = list(kf.split(all_positions))\n",
    "    \n",
    "    # Deduplicate classical model data\n",
    "    classical_dedup_probs, classical_dedup_beliefs, classical_dedup_indices, classical_prefix_to_indices = deduplicate_data(\n",
    "        classical_nn_inputs, \n",
    "        classical_nn_probs, \n",
    "        classical_nn_beliefs\n",
    "    )\n",
    "\n",
    "    classical_kf, classical_all_positions = compute_kfold_split(classical_dedup_probs, n_splits=100, random_state=42)\n",
    "    classical_kf_list = list(classical_kf.split(classical_all_positions))\n",
    "\n",
    "    ground_truth_data = defaultdict(dict) # Keys: ckpt -> layer -> predictions\n",
    "    ground_truth_data['probs'] = np.array(dedup_probs)\n",
    "    ground_truth_data['beliefs'] = np.array(dedup_beliefs)\n",
    "    ground_truth_data['indices'] = np.array(dedup_indices, dtype=object)\n",
    "    joblib.dump(ground_truth_data, f'{run_dir}/ground_truth_data.joblib')\n",
    "\n",
    "    classical_ground_truth_data = defaultdict(dict) # Keys: ckpt -> layer -> predictions\n",
    "    classical_ground_truth_data['probs'] = np.array(classical_dedup_probs)\n",
    "    classical_ground_truth_data['beliefs'] = np.array(classical_dedup_beliefs)\n",
    "    classical_ground_truth_data['indices'] = np.array(classical_dedup_indices, dtype=object)\n",
    "    joblib.dump(classical_ground_truth_data, f'{run_dir}/markov3_ground_truth_data.joblib')\n",
    "    \n",
    "    checkpoints = s3_loader.list_checkpoints(sweep, run_id)\n",
    "    for epoch, ckpt in enumerate(tqdm(checkpoints)):\n",
    "        model, run_config = s3_loader.load_checkpoint(sweep, run_id, ckpt)\n",
    "\n",
    "        #ckpt_ind is between / and .pt\n",
    "        ckpt_ind = ckpt.split('/')[-1].split('.')[0]\n",
    "        # we want the value of 'val_loss_mean' where num_tokens_seen == ckpt_ind\n",
    "        try:\n",
    "            filtered_df = loss_df[loss_df['epoch'] == epoch-1]\n",
    "            if len(filtered_df) > 0 and 'val_loss_mean' in filtered_df.columns:\n",
    "                val_loss_mean = filtered_df['val_loss_mean'].values[0]\n",
    "            else:\n",
    "                val_loss_mean = float('nan')\n",
    "        except (KeyError, IndexError, AttributeError):\n",
    "            val_loss_mean = float('nan')\n",
    "        \n",
    "        act_extractor = ActivationExtractor(device='cpu')\n",
    "        nn_acts_ = act_extractor.extract_activations(\n",
    "            model,\n",
    "            nn_inputs,\n",
    "            get_nn_type(run_id),\n",
    "            relevant_activation_keys=TRANSFORMER_ACTIVATION_KEYS,\n",
    "        )\n",
    "        nn_acts = {}\n",
    "        for layer, acts in nn_acts_.items():\n",
    "            nn_acts[layer] = acts\n",
    "        nn_acts['combined'] = _combine_layer_activations(nn_acts)\n",
    "\n",
    "        classical_acts_ = act_extractor.extract_activations(\n",
    "            model,\n",
    "            classical_nn_inputs,\n",
    "            get_nn_type(run_id),\n",
    "            relevant_activation_keys=TRANSFORMER_ACTIVATION_KEYS,\n",
    "        )\n",
    "        classical_acts = {}\n",
    "        for layer, acts in classical_acts_.items():\n",
    "            classical_acts[layer] = acts\n",
    "        classical_acts['combined'] = _combine_layer_activations(classical_acts)\n",
    "        \n",
    "        save_data = collections.defaultdict(nested_dict_factory) # Use the named function here\n",
    "        classical_save_data = collections.defaultdict(nested_dict_factory) # Use the named function here\n",
    "        \n",
    "        for layer, act in nn_acts.items():\n",
    "            \n",
    "            # dedup the activations\n",
    "            dedup_acts, dedup_indices = deduplicate_tensor(prefix_to_indices, act, aggregation_fn=None)\n",
    "\n",
    "            classical_dedup_acts, classical_dedup_indices = deduplicate_tensor(classical_prefix_to_indices, classical_acts[layer], aggregation_fn=None)\n",
    "            \n",
    "            zscore_acts = (dedup_acts.numpy() - dedup_acts.numpy().mean(axis=0)) / dedup_acts.numpy().std(axis=0)\n",
    "            cum_var_exp, _, _ = calculate_weighted_pca_variance(dedup_acts.numpy(), dedup_probs.numpy())\n",
    "            cum_var_exp_zscore, _, _ = calculate_weighted_pca_variance(zscore_acts, dedup_probs.numpy())\n",
    "\n",
    "            classical_zscore_acts = (classical_dedup_acts.numpy() - classical_dedup_acts.numpy().mean(axis=0)) / classical_dedup_acts.numpy().std(axis=0)\n",
    "            classical_cum_var_exp, _, _ = calculate_weighted_pca_variance(classical_dedup_acts.numpy(), classical_dedup_probs.numpy())\n",
    "            classical_cum_var_exp_zscore, _, _ = calculate_weighted_pca_variance(classical_zscore_acts, classical_dedup_probs.numpy())\n",
    "\n",
    "\n",
    "\n",
    "            results =run_activation_to_beliefs_regression_kf(\n",
    "                reg_analyzer,\n",
    "                dedup_acts,\n",
    "                dedup_beliefs,\n",
    "                dedup_probs,\n",
    "                kf_list,\n",
    "                rcond_values=RCOND_SWEEP_LIST,\n",
    "            )\n",
    "\n",
    "            classical_results = run_activation_to_beliefs_regression_kf(\n",
    "                reg_analyzer,\n",
    "                classical_dedup_acts,\n",
    "                classical_dedup_beliefs,\n",
    "                classical_dedup_probs,\n",
    "                classical_kf_list,\n",
    "                rcond_values=RCOND_SWEEP_LIST,\n",
    "            )\n",
    "\n",
    "            # Calculate Euclidean distance weighted by probabilities\n",
    "            save_data[layer]['predicted_beliefs'] = results['predictions']\n",
    "            save_data[layer]['rmse'] = results['final_metrics']['rmse']\n",
    "            save_data[layer]['mae'] = results['final_metrics']['mae']\n",
    "            save_data[layer]['r2'] = results['final_metrics']['r2']\n",
    "            save_data[layer]['dist'] = results['final_metrics']['dist']\n",
    "            save_data[layer]['mse'] = results['final_metrics']['mse']\n",
    "            save_data[layer]['cum_var_exp'] = cum_var_exp\n",
    "            save_data[layer]['cum_var_exp_zscore'] = cum_var_exp_zscore\n",
    "            save_data[layer]['val_loss_mean'] = val_loss_mean\n",
    "\n",
    "            # Calculate Euclidean distance weighted by probabilities\n",
    "            \n",
    "            # Only save predictions for epoch 0 or final epoch to save space\n",
    "            if epoch == 0 or epoch == len(checkpoints) - 1:\n",
    "                classical_save_data[layer]['predicted_beliefs'] = classical_results['predictions']\n",
    "            else:\n",
    "                classical_save_data[layer]['predicted_beliefs'] = None  # Skip storing predictions for intermediate epochs\n",
    "            classical_save_data[layer]['rmse'] = classical_results['final_metrics']['rmse']\n",
    "            classical_save_data[layer]['mae'] = classical_results['final_metrics']['mae']\n",
    "            classical_save_data[layer]['r2'] = classical_results['final_metrics']['r2']\n",
    "            classical_save_data[layer]['dist'] = classical_results['final_metrics']['dist']\n",
    "            classical_save_data[layer]['mse'] = classical_results['final_metrics']['mse']\n",
    "            classical_save_data[layer]['cum_var_exp'] = classical_cum_var_exp\n",
    "            classical_save_data[layer]['cum_var_exp_zscore'] = classical_cum_var_exp_zscore\n",
    "            classical_save_data[layer]['val_loss_mean'] = val_loss_mean\n",
    "        \n",
    "        # Save each checkpoint's data to a separate file\n",
    "        joblib.dump(save_data, f'{run_dir}/checkpoint_{ckpt_ind}.joblib')\n",
    "        joblib.dump(classical_save_data, f'{run_dir}/markov3_checkpoint_{ckpt_ind}.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epsilon-machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
